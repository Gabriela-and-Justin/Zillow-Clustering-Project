{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Scratchpad for Zillow Clustering Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import acquire\n",
    "import prepare\n",
    "import explore\n",
    "from env import host, user, password\n",
    "\n",
    "#Visualization Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Sklearn Tools and Modules\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire Zillow Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>typeconstructiontypeid</th>\n",
       "      <th>storytypeid</th>\n",
       "      <th>heatingorsystemtypeid</th>\n",
       "      <th>buildingclasstypeid</th>\n",
       "      <th>architecturalstyletypeid</th>\n",
       "      <th>airconditioningtypeid</th>\n",
       "      <th>parcelid</th>\n",
       "      <th>id</th>\n",
       "      <th>basementsqft</th>\n",
       "      <th>bathroomcnt</th>\n",
       "      <th>...</th>\n",
       "      <th>taxdelinquencyyear</th>\n",
       "      <th>censustractandblock</th>\n",
       "      <th>logerror</th>\n",
       "      <th>transactiondate</th>\n",
       "      <th>airconditioningdesc</th>\n",
       "      <th>architecturalstyledesc</th>\n",
       "      <th>buildingclassdesc</th>\n",
       "      <th>heatingorsystemdesc</th>\n",
       "      <th>storydesc</th>\n",
       "      <th>typeconstructiondesc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10711855</td>\n",
       "      <td>1087254</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.037113e+13</td>\n",
       "      <td>-0.007357</td>\n",
       "      <td>2017-07-07</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Central</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10711877</td>\n",
       "      <td>1072280</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.037113e+13</td>\n",
       "      <td>0.021066</td>\n",
       "      <td>2017-08-29</td>\n",
       "      <td>Central</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Central</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10711888</td>\n",
       "      <td>1340933</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.037113e+13</td>\n",
       "      <td>0.077174</td>\n",
       "      <td>2017-04-04</td>\n",
       "      <td>Central</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Central</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10711910</td>\n",
       "      <td>1878109</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.037113e+13</td>\n",
       "      <td>-0.041238</td>\n",
       "      <td>2017-03-17</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Central</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10711923</td>\n",
       "      <td>2190858</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.037113e+13</td>\n",
       "      <td>-0.009496</td>\n",
       "      <td>2017-03-24</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Central</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   typeconstructiontypeid  storytypeid  heatingorsystemtypeid  \\\n",
       "0                     NaN          NaN                    2.0   \n",
       "1                     NaN          NaN                    2.0   \n",
       "2                     NaN          NaN                    2.0   \n",
       "3                     NaN          NaN                    2.0   \n",
       "4                     NaN          NaN                    2.0   \n",
       "\n",
       "   buildingclasstypeid  architecturalstyletypeid  airconditioningtypeid  \\\n",
       "0                  NaN                       NaN                    NaN   \n",
       "1                  NaN                       NaN                    1.0   \n",
       "2                  NaN                       NaN                    1.0   \n",
       "3                  NaN                       NaN                    NaN   \n",
       "4                  NaN                       NaN                    NaN   \n",
       "\n",
       "   parcelid       id  basementsqft  bathroomcnt  ...  taxdelinquencyyear  \\\n",
       "0  10711855  1087254           NaN          2.0  ...                 NaN   \n",
       "1  10711877  1072280           NaN          2.0  ...                 NaN   \n",
       "2  10711888  1340933           NaN          2.0  ...                 NaN   \n",
       "3  10711910  1878109           NaN          2.0  ...                 NaN   \n",
       "4  10711923  2190858           NaN          2.0  ...                 NaN   \n",
       "\n",
       "   censustractandblock  logerror  transactiondate  airconditioningdesc  \\\n",
       "0         6.037113e+13 -0.007357       2017-07-07                 None   \n",
       "1         6.037113e+13  0.021066       2017-08-29              Central   \n",
       "2         6.037113e+13  0.077174       2017-04-04              Central   \n",
       "3         6.037113e+13 -0.041238       2017-03-17                 None   \n",
       "4         6.037113e+13 -0.009496       2017-03-24                 None   \n",
       "\n",
       "   architecturalstyledesc  buildingclassdesc  heatingorsystemdesc  storydesc  \\\n",
       "0                    None               None              Central       None   \n",
       "1                    None               None              Central       None   \n",
       "2                    None               None              Central       None   \n",
       "3                    None               None              Central       None   \n",
       "4                    None               None              Central       None   \n",
       "\n",
       "   typeconstructiondesc  \n",
       "0                  None  \n",
       "1                  None  \n",
       "2                  None  \n",
       "3                  None  \n",
       "4                  None  \n",
       "\n",
       "[5 rows x 67 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = acquire.get_zillow_data(cached=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77413, 67)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Zillow Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prepare.wrangle_zillow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69760, 21)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['bathroomcnt', 'bedroomcnt', 'buildingqualitytypeid',\n",
       "       'calculatedfinishedsquarefeet', 'latitude', 'longitude',\n",
       "       'rawcensustractandblock', 'regionidcity', 'regionidzip',\n",
       "       'structuretaxvaluedollarcnt', 'landtaxvaluedollarcnt', 'logerror', 'LA',\n",
       "       'Orange', 'Ventura', 'age', 'taxrate', 'acres', 'cola',\n",
       "       'hot_month_sale', 'has_heat'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 69760 entries, 10711855 to 162960814\n",
      "Data columns (total 21 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   bathroomcnt                   69760 non-null  float64\n",
      " 1   bedroomcnt                    69760 non-null  float64\n",
      " 2   buildingqualitytypeid         69760 non-null  float64\n",
      " 3   calculatedfinishedsquarefeet  69760 non-null  float64\n",
      " 4   latitude                      69760 non-null  float64\n",
      " 5   longitude                     69760 non-null  float64\n",
      " 6   rawcensustractandblock        69760 non-null  float64\n",
      " 7   regionidcity                  69760 non-null  float64\n",
      " 8   regionidzip                   69760 non-null  float64\n",
      " 9   structuretaxvaluedollarcnt    69760 non-null  float64\n",
      " 10  landtaxvaluedollarcnt         69760 non-null  float64\n",
      " 11  logerror                      69760 non-null  float64\n",
      " 12  LA                            69760 non-null  uint8  \n",
      " 13  Orange                        69760 non-null  uint8  \n",
      " 14  Ventura                       69760 non-null  uint8  \n",
      " 15  age                           69760 non-null  float64\n",
      " 16  taxrate                       69760 non-null  float64\n",
      " 17  acres                         69760 non-null  float64\n",
      " 18  cola                          69760 non-null  int64  \n",
      " 19  hot_month_sale                69760 non-null  int64  \n",
      " 20  has_heat                      69760 non-null  int64  \n",
      "dtypes: float64(15), int64(3), uint8(3)\n",
      "memory usage: 10.3 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counties():\n",
    "    '''\n",
    "    This function will create dummy variables out of the original fips column. \n",
    "    And return a dataframe with all of the original columns except regionidcounty.\n",
    "    We will keep fips column for data validation after making changes. \n",
    "    New columns added will be 'LA', 'Orange', and 'Ventura' which are boolean \n",
    "    The fips ids are renamed to be the name of the county each represents. \n",
    "    '''\n",
    "    # create dummy vars of fips id\n",
    "    county_df = pd.get_dummies(df.fips)\n",
    "    # rename columns by actual county name\n",
    "    county_df.columns = ['LA', 'Orange', 'Ventura']\n",
    "    # concatenate the dataframe with the 3 county columns to the original dataframe\n",
    "    df_dummies = pd.concat([df, county_df], axis = 1)\n",
    "    # drop regionidcounty and fips columns\n",
    "    df_dummies = df_dummies.drop(columns = ['regionidcounty'])\n",
    "    return df_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_counties()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.roomcnt.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decisions / To-Do List:\n",
    "- Add get_counties function to prepare module and update wrangle_zillow\n",
    "- \n",
    "\n",
    "#### Questions:\n",
    "- Why are there 0s in roomcnt? - Is this column valuable?\n",
    "- Do I need to keep County and FIPS after encoding? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    df['age'] = 2017 - df.yearbuilt\n",
    "    df['age_bin'] = pd.cut(df.age, \n",
    "                           bins = [0, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140],\n",
    "                           labels = [0, .066, .133, .20, .266, .333, .40, .466, .533, \n",
    "                                     .60, .666, .733, .8, .866, .933])\n",
    "\n",
    "    # create taxrate variable\n",
    "    df['taxrate'] = df.taxamount/df.taxvaluedollarcnt*100\n",
    "\n",
    "    # create acres variable\n",
    "    df['acres'] = df.lotsizesquarefeet/43560\n",
    "\n",
    "    # bin acres\n",
    "    df['acres_bin'] = pd.cut(df.acres, bins = [0, .10, .15, .25, .5, 1, 5, 10, 20, 50, 200], \n",
    "                       labels = [0, .1, .2, .3, .4, .5, .6, .7, .8, .9])\n",
    "\n",
    "    # square feet bin\n",
    "    df['sqft_bin'] = pd.cut(df.calculatedfinishedsquarefeet, \n",
    "                            bins = [0, 800, 1000, 1250, 1500, 2000, 2500, 3000, 4000, 7000, 12000],\n",
    "                            labels = [0, .1, .2, .3, .4, .5, .6, .7, .8, .9]\n",
    "                       )\n",
    "\n",
    "    # dollar per square foot-structure\n",
    "    df['structure_dollar_per_sqft'] = df.structuretaxvaluedollarcnt/df.calculatedfinishedsquarefeet\n",
    "\n",
    "\n",
    "    df['structure_dollar_sqft_bin'] = pd.cut(df.structure_dollar_per_sqft, \n",
    "                                             bins = [0, 25, 50, 75, 100, 150, 200, 300, 500, 1000, 1500],\n",
    "                                             labels = [0, .1, .2, .3, .4, .5, .6, .7, .8, .9]\n",
    "                                            )\n",
    "\n",
    "\n",
    "    # dollar per square foot-land\n",
    "    df['land_dollar_per_sqft'] = df.landtaxvaluedollarcnt/df.lotsizesquarefeet\n",
    "\n",
    "    df['lot_dollar_sqft_bin'] = pd.cut(df.land_dollar_per_sqft, bins = [0, 1, 5, 20, 50, 100, 250, 500, 1000, 1500, 2000],\n",
    "                                       labels = [0, .1, .2, .3, .4, .5, .6, .7, .8, .9]\n",
    "                                      )\n",
    "\n",
    "\n",
    "    # update datatypes of binned values to be float\n",
    "    df = df.astype({'sqft_bin': 'float64', 'acres_bin': 'float64', 'age_bin': 'float64',\n",
    "                    'structure_dollar_sqft_bin': 'float64', 'lot_dollar_sqft_bin': 'float64'})\n",
    "\n",
    "\n",
    "    # ratio of bathrooms to bedrooms\n",
    "    df['bath_bed_ratio'] = df.bathroomcnt/df.bedroomcnt\n",
    "\n",
    "    # 12447 is the ID for city of LA. \n",
    "    # I confirmed through sampling and plotting, as well as looking up a few addresses.\n",
    "    df['cola'] = df['regionidcity'].apply(lambda x: 1 if x == 12447.0 else 0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns={'fips','roomcnt','unitcnt', 'assessmentyear','logerror_class','county'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Zillow Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for outliers in Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explore Dists of Features:\n",
    "for col in df.columns:\n",
    "    sns.boxplot(df[col])\n",
    "    plt.title(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaways:\n",
    "- Should I deal with outliers in Bathroom count? This was a strong feature/ reliable in regression project without removing outliers of bathroom count directly.\n",
    "- Outliers in bedroom count as well...Make a feature bedrooms/bathrooms?\n",
    "- Building Quality type id looks centered / normally distributed\n",
    "- Outliers in finished square feet \n",
    "- Outlier in FIPS? Is that Ventura County? \n",
    "- Lat and Long look okay...no action for now\n",
    "- Lotsize square feet has outliers on upperbound\n",
    "- Rawcensustractandblock...still need to determine what this is\n",
    "- Regionid City...Most properties in LA?\n",
    "- Regionzip...still want to deal with this...suspect errors in data\n",
    "- roomcnt...most are 0? It is total number of rooms in principal of residence...Should just drop\n",
    "- Unit count is as expected...drop since all values are equal\n",
    "- Year built has some outliers...should create new feature for age of home? \n",
    "- Structuretaxvaluedollar cnt has outliers in upperbound \n",
    "- Assessment year is the same for all properties. Drop column\n",
    "- Landtax value dollar cnt has has upperbound outliers...Do these outliers correspond with sturcture tax value dollar count?\n",
    "- Logerror has outliers..How do I want to handle this? Leave as is for the first iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_label(x, lower, upper):\n",
    "    if (lower >= x) or (x >= upper):\n",
    "        return 'Yes'\n",
    "    else:\n",
    "        \n",
    "        return 'No'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_check = ['bathroomcnt', 'bedroomcnt',\n",
    "       'calculatedfinishedsquarefeet',\n",
    "       'lotsizesquarefeet', 'rawcensustractandblock','yearbuilt',\n",
    "       'structuretaxvaluedollarcnt', 'taxvaluedollarcnt',\n",
    "       'landtaxvaluedollarcnt', 'taxamount', 'logerror']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in columns_to_check:\n",
    "    quartile_01, quartile_03 = np.percentile(df[col], [25, 75])\n",
    "    iqr = quartile_03 - quartile_01\n",
    "    \n",
    "    lower_bound = quartile_01 -(3 * iqr)\n",
    "    upper_bound = quartile_03 +(3 * iqr)\n",
    "    \n",
    "    df['outlier'] = df[col].apply(lambda x: outlier_label(x, lower_bound, upper_bound))\n",
    "    \n",
    "\n",
    "    print(f\"The lower and upper bound of the range for '{col}' respectively is: {lower_bound} and {upper_bound}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = df['outlier'] == 'Yes' # filter for outliers\n",
    "df[outliers][['bathroomcnt', 'bedroomcnt',\n",
    "       'calculatedfinishedsquarefeet',\n",
    "       'lotsizesquarefeet', 'rawcensustractandblock','yearbuilt',\n",
    "       'structuretaxvaluedollarcnt', 'taxvaluedollarcnt',\n",
    "       'landtaxvaluedollarcnt', 'taxamount', 'logerror']].groupby('parcelid').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many total outliers are there in the dataframe as defined by the outlier function above?\n",
    "len(df[df.outlier == 'Yes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The % of observations lost by removing all outliers is: {69920/3752}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If I just remove all outliers in the features listed above in columns_to_check, I would lose 18.63 % of the data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    plt.hist(df[col])\n",
    "    plt.title(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Takeaways:**\n",
    "- Need to deal with outliers in \n",
    "    - calculatedfinishedsquarefeet\n",
    "    - lotsizesquarefeet\n",
    "    - taxdollarvaluecnt\n",
    "- roomcnt has errors - need to drop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers():\n",
    "    '''\n",
    "    remove outliers in bed, bath, square feet\n",
    "    '''\n",
    "\n",
    "    return df[((df.bathroomcnt <= 7) & (df.bedroomcnt <= 11) & \n",
    "               (df.bathroomcnt > 0) & \n",
    "               (df.bedroomcnt > 0) & \n",
    "               (df.calculatedfinishedsquarefeet < 4717) \n",
    "               )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = remove_outliers()\n",
    "\n",
    "# Now that the most extreme outliers have been removed, let's look at the summary statistics of each numeric field. \n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now I am going to break up the dataframe into three separate dfs by county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create LA County Dataframe\n",
    "df_la = df[df.LA == 1].drop(columns = ['bedroomcnt', 'taxamount', 'taxvaluedollarcnt', \n",
    "                                       'structure_dollar_per_sqft', 'land_dollar_per_sqft', 'yearbuilt', \n",
    "                                       'lotsizesquarefeet', 'regionidcity', 'regionidzip', \n",
    "                                       'structuretaxvaluedollarcnt', 'landtaxvaluedollarcnt', \n",
    "                                       'LA', 'Ventura', 'Orange']) \n",
    "df_la.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Ventura County Dataframe\n",
    "df_vent = df[df.Ventura == 1].drop(columns = ['bedroomcnt', 'taxamount', 'taxvaluedollarcnt', \n",
    "                                       'structure_dollar_per_sqft', 'land_dollar_per_sqft', 'yearbuilt', \n",
    "                                       'lotsizesquarefeet', 'regionidcity', 'regionidzip', \n",
    "                                       'structuretaxvaluedollarcnt', 'landtaxvaluedollarcnt', \n",
    "                                       'LA', 'Ventura', 'Orange']) \n",
    "df_vent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Orange County Dataframe\n",
    "df_orange = df[df.Orange == 1].drop(columns = ['bedroomcnt', 'taxamount', 'taxvaluedollarcnt', \n",
    "                                       'structure_dollar_per_sqft', 'land_dollar_per_sqft', 'yearbuilt', \n",
    "                                       'lotsizesquarefeet', 'regionidcity', 'regionidzip', \n",
    "                                       'structuretaxvaluedollarcnt', 'landtaxvaluedollarcnt', \n",
    "                                       'LA', 'Ventura', 'Orange']) \n",
    "df_orange.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Starting with Analysis for LA County Props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(df, target_var):\n",
    "    '''\n",
    "    This function takes in the dataframe and target variable name as arguments and then\n",
    "    splits the dataframe into train (56%), validate (24%), & test (20%)\n",
    "    It will return a list containing the following dataframes: train (for exploration), \n",
    "    X_train, X_validate, X_test, y_train, y_validate, y_test\n",
    "    '''\n",
    "    # split df into train_validate (80%) and test (20%)\n",
    "    train_validate, test = train_test_split(df, test_size=.20, random_state=13)\n",
    "    # split train_validate into train(70% of 80% = 56%) and validate (30% of 80% = 24%)\n",
    "    train, validate = train_test_split(train_validate, test_size=.3, random_state=13)\n",
    "\n",
    "    # create X_train by dropping the target variable \n",
    "    X_train = train.drop(columns=[target_var])\n",
    "    # create y_train by keeping only the target variable.\n",
    "    y_train = train[[target_var]]\n",
    "\n",
    "    # create X_validate by dropping the target variable \n",
    "    X_validate = validate.drop(columns=[target_var])\n",
    "    # create y_validate by keeping only the target variable.\n",
    "    y_validate = validate[[target_var]]\n",
    "\n",
    "    # create X_test by dropping the target variable \n",
    "    X_test = test.drop(columns=[target_var])\n",
    "    # create y_test by keeping only the target variable.\n",
    "    y_test = test[[target_var]]\n",
    "\n",
    "    partitions = [train, X_train, X_validate, X_test, y_train, y_validate, y_test]\n",
    "    return partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions = split(df_la, target_var='logerror')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = partitions[0]\n",
    "train['logerror_bins'] = pd.cut(train.logerror, [-5, -.2, -.05, .05, .2, 4])\n",
    "partitions[0] = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.logerror_bins.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bivariate Analysis on train for LA County"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate logerror into quantiles\n",
    "train['logerror_class'] = pd.qcut(train.logerror, q=4, labels=['q1', 'q2', 'q3', 'q4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_target = ['logerror_class']\n",
    "continuous_target = ['logerror']\n",
    "binary_vars = ['cola']\n",
    "quant_vars = ['bathroomcnt', 'buildingqualitytypeid', 'calculatedfinishedsquarefeet',\n",
    "       'latitude', 'longitude', 'rawcensustractandblock',\n",
    "       'transactiondate', 'heatingorsystemdesc', 'age', 'age_bin', 'taxrate',\n",
    "       'acres', 'acres_bin', 'sqft_bin', 'structure_dollar_sqft_bin',\n",
    "       'lot_dollar_sqft_bin', 'bath_bed_ratio', 'cola', 'logerror_bins']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore.explore_bivariate(train, categorical_target, continuous_target, binary_vars, quant_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multivariate Analysis\n",
    "#sns.pairplot(data = train, hue = 'logerror_bins', \n",
    "#             x_vars = ['logerror', 'structuretaxvaluedollarcnt','landtaxvaluedollarcnt', 'taxvaluedollarcnt', 'taxamount'],\n",
    "#             y_vars = ['logerror', 'bathroomcnt', 'bedroomcnt', 'calculatedfinishedsquarefeet', 'yearbuilt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale Continuous Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add this column to the above drop columns list\n",
    "train.drop(columns={'transactiondate'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns={'logerror_class'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the variables that still need scaling\n",
    "scaled_vars = ['bathroomcnt', 'bedroomcnt', 'buildingqualitytypeid',\n",
    "       'calculatedfinishedsquarefeet', 'latitude', 'longitude',\n",
    "       'lotsizesquarefeet', 'rawcensustractandblock', 'regionidcity',\n",
    "       'regionidzip', 'yearbuilt', 'structuretaxvaluedollarcnt',\n",
    "       'taxvaluedollarcnt', 'landtaxvaluedollarcnt', 'taxamount']\n",
    "\n",
    "# create new column names for the scaled variables by adding 'scaled_' to the beginning of each variable name \n",
    "scaled_column_names = ['scaled_' + i for i in scaled_vars]\n",
    "\n",
    "# select the X partitions: [X_train, X_validate, X_test]\n",
    "X = partitions[1:4]\n",
    "\n",
    "# fit the minmaxscaler to X_train\n",
    "X_train = X[0]\n",
    "scaler = MinMaxScaler(copy=True).fit(X_train[scaled_vars])\n",
    "\n",
    "\n",
    "def scale_and_concat(df):\n",
    "    scaled_array = scaler.transform(df[scaled_vars])\n",
    "    scaled_df = pd.DataFrame(scaled_array, columns=scaled_column_names, index=df.index.values)\n",
    "    return pd.concat((df, scaled_df), axis=1)\n",
    "\n",
    "for i in range(len(X)):\n",
    "    X[i] = scale_and_concat(X[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall: X[0] is X_train, X[1] is X_validate and X[2] is X_test\n",
    "X[0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of variables I will cluster on. \n",
    "cluster_vars = ['scaled_latitude', 'scaled_longitude', 'scaled_yearbuilt']\n",
    "cluster_name = 'area_cluster'\n",
    "k_range = range(2,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_k(X_train, cluster_vars, k_range):\n",
    "    sse = []\n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k)\n",
    "\n",
    "        # X[0] is our X_train dataframe..the first dataframe in the list of dataframes stored in X. \n",
    "        kmeans.fit(X_train[cluster_vars])\n",
    "\n",
    "        # inertia: Sum of squared distances of samples to their closest cluster center.\n",
    "        sse.append(kmeans.inertia_) \n",
    "\n",
    "    # compute the difference from one k to the next\n",
    "    delta = [round(sse[i] - sse[i+1],0) for i in range(len(sse)-1)]\n",
    "\n",
    "    # compute the percent difference from one k to the next\n",
    "    pct_delta = [round(((sse[i] - sse[i+1])/sse[i])*100, 1) for i in range(len(sse)-1)]\n",
    "\n",
    "    # create a dataframe with all of our metrics to compare them across values of k: SSE, delta, pct_delta\n",
    "    k_comparisons_df = pd.DataFrame(dict(k=k_range[0:-1], \n",
    "                             sse=sse[0:-1], \n",
    "                             delta=delta, \n",
    "                             pct_delta=pct_delta))\n",
    "\n",
    "    # plot k with inertia\n",
    "    plt.plot(k_comparisons_df.k, k_comparisons_df.sse, 'bx-')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('SSE')\n",
    "    plt.title('The Elbow Method to find the optimal k\\nFor which k values do we see large decreases in SSE?')\n",
    "    plt.show()\n",
    "\n",
    "    # plot k with pct_delta\n",
    "    plt.plot(k_comparisons_df.k, k_comparisons_df.pct_delta, 'bx-')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Percent Change')\n",
    "    plt.title('For which k values are we seeing increased changes (%) in SSE?')\n",
    "    plt.show()\n",
    "\n",
    "    # plot k with delta\n",
    "    plt.plot(k_comparisons_df.k, k_comparisons_df.delta, 'bx-')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Absolute Change in SSE')\n",
    "    plt.title('For which k values are we seeing increased changes (absolute) in SSE?')\n",
    "    plt.show()\n",
    "\n",
    "    return k_comparisons_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_k(X[0], cluster_vars, k_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
